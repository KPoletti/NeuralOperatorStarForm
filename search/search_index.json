{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NeuralOperatorStarForm's documentation","text":""},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>main.py                     # The main script that runs the neural operator.\ninput.py                    # The configuration file.\nsweep.py                    # The script that runs the sweeps.\nsrc/\n    dissipative_utils.py    # Functions for the markov neural operator encoder.\n    GenerateSweep.py        # Script to generate a sweep config file for wandb.\n    meta_dataset.py         # pytorch dataset class that allows the passing of metadata.\n    networkUtils.py         # Functions for the neural networks.\n    train.py                # Class to train the neural operator.\n    utils.py                # Utility functions for preparing the dataset.\n</code></pre>"},{"location":"about/","title":"About","text":"<p>Uses multiple Neural Operators to approximate stellar formation in IMF.</p>"},{"location":"dataset/","title":"Datasets","text":""},{"location":"dataset/#2-d-incompressible-navier-stokes","title":"2-D Incompressible Navier-Stokes","text":"<p>Researchers at Caltech simulated Navier stokes on</p> <ul> <li>KF-Re100 - never used this dataset</li> <li>Navier-Stokes - 2-D incompressible Navier-Stokes with viscosity 0.001, with shape of  (# of trajectories, grid_x, gird_y, timestep per trajectory)= (5000, 64, 64, 51)</li> </ul>"},{"location":"dataset/#cats-mhd-sim","title":"CATS MHD Sim","text":"<p>From CATS MHD Simulations, shape of (# of trajectories, grid_x, gird_y, timestep per trajectory)=[2970, 256, 256, 2]</p>"},{"location":"dataset/#mhd-turbulence","title":"MHD Turbulence","text":"<p>Simulation of turbulent MHD with shape of (# of trajectories, grid_x, gird_y, timesteps per trajectory). Each dataset has multiple projections of the same simulation in different directions.</p> Dataset Shape filename Mu2 [874, 800, 800, 2] <code>density_mu2_dN10</code> Mu8 [875, 800, 800, 2] <code>density_mu8_dN10</code> Mu32 [954, 800, 800, 2] <code>density_mu32_dN10</code> Mu0 [774, 800, 800, 2] <code>density_mu0_dN10</code>"},{"location":"dataset/#gravitational-collapse","title":"Gravitational Collapse","text":"<p>GIZMO simulation of spherically symmetric gas cloud with different virial parameters by varying the initial mass.   Filenames are of the form</p> <pre><code>f\"Grav_M{mass in solar masses}_dN{timesteps per trajectory}_dt{size of timestep in kyr}.pt\"\n</code></pre> <p>If the file ends with <code>multiData</code>, then data is for density and velocity in the x and y directions. Otherwise, the file contains only density. Most files have the shape of [30, 800, 800, 3, dN], where dN is the number of timesteps per trajectory. The exceptions are files that start with <code>Grav_MALL</code> which have all the data in one file. They are the shape of [902, g, g, 3, dN], where g is the grid size and is either 800 or 200.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.6</li> <li>PyTorch &gt;= 1.6.0</li> <li>CUDA &gt;= 10.1</li> <li>GCC &gt;= 4.9</li> </ul>"},{"location":"input_file/","title":"Input File","text":"<p>The input file is a python that specifies the configures the dataset, the neural network, and the training parameters. Here is an overview of the input file format.</p> Example input.py input.py<pre><code>import torch.nn as nn\nfrom src.dissipative_utils import (\n    sample_uniform_spherical_shell,\n    linear_scale_dissipative_target,\n)\nfrom neuralop import LpLoss, H1Loss  # type: ignore\nimport math\n\nlevel = \"DEBUG\"  # DEBUG, INFO, WARNING, ERROR, CRITICAL\ndata_name = \"GravColl\"  # NS-Caltech, StarForm, GravColl, GravInts or CATS\nloss_name = \"H1Loss\"  # LpLoss, H1Loss\n\nlog = False  # Option to take the log of the data\ndoPlot = True  # Option to create plots\nuse_mlp = True  # Option to use MLP\nencoder = False  # Option to use the encoder\nuse_ddp = False  # Option to use multi-gpu distributed data\n\ndiss_reg = False\npreactivation = True  # Option to use ResNet Preactivation\nsaveNeuralNetwork = True  # Option to save the neural network\npositional_encoding = False  # TODO: Debug FNO3d with position encoding\nif positional_encoding:\n    grid_boundaries = [[-1.25, 1.25], [-1.25, 1.25]]\nn_layers = 4\nmlp_dropout = 0.01\n# Pooling parameters\npoolKernel = 0  # set to 0 to disable pooling\npoolStride = 0  # set to 0 to disable pooling\n#############################################\n# FOR NS Cal-tech Data\n#############################################\nif data_name == \"NS-Caltech\":\n    DATA_PATH = \"../dataToSend/CaltechData/\"\n    TRAIN_PATH = f\"{DATA_PATH}ns_V1e-3_N5000_T50.pt\"\n    TIME_PATH = f\"{DATA_PATH}ns_V1e-3_N5000_T50.h5\"\n\n    S = 64\n    N = 5000\n\n    modes = 12  # star Form 20\n    width = 24  # star Form 100\n    input_channels = 10\n    output_channels = 10\n\n    T = 10\n    T_in = 10\n    poolKernel = 0  # set to 0 to disable pooling\n    poolStride = 0  # set to 0 to disable pooling\n\n##############################################\n# For Grav Collapse\n##############################################\nelif data_name == \"GravColl\":\n    S = 128\n    T = 5\n    T_in = 5\n    T_out = 14\n    DATA_PATH = \"../dataToSend/TrainingData/LowRes/\"\n    dN = 10\n    mass = \"ALL\"\n    # dt = 0.0204\n    extras = \"_multiData\"\n\n    TRAIN_PATH = f\"{DATA_PATH}Grav_M{mass}_dN{dN}{extras}.pt\"\n    TIME_PATH = f\"{DATA_PATH}Grav_M{mass}_dN{dN}{extras}.h5\"\n    N = 29\n    if mass == \"ALL\":\n        N = 902\n    data_name = f\"{data_name}{mass}_dN{dN}\"\n    input_channels = 3\n    output_channels = 3\n    modes = 4  # star Form 20\n    width = 10  # star Form 100\n    poolKernel = 2  # set to 0 to disable pooling\n    poolStride = 2  # set to 0 to disable pooling\n\nelif data_name == \"Turb\":\n    N = 4664\n    S = 64\n    dN = 10\n    T = 5\n    T_in = 5\n    T_out = 5\n\n    mass = \"12ALL\"\n    extras = \"_Seed\"\n    DATA_PATH = \"../dataToSend/TrainingData/TurbSeed/\"\n    TRAIN_PATH = f\"{DATA_PATH}Turb_VP{mass}_dN{dN}{extras}.pt\"\n    TIME_PATH = f\"{DATA_PATH}Turb_VP{mass}_dN{dN}{extras}.h5\"\n    # mass = \"1-4\"\n    # extras = \"_SmallRange\"\n    # DATA_PATH = \"../dataToSend/TrainingData/TurbProj_SmallerVP/\"\n    # TRAIN_PATH = f\"{DATA_PATH}Turb_VP{mass}_dN{dN}{extras}.pt\"\n    # TIME_PATH = f\"{DATA_PATH}Turb_VP{mass}_dN{dN}{extras}.h5\"\n    data_name = f\"{data_name}_VP{mass}_dN{dN}\"\n\n    modes = 8  # star Form 20\n    width = 16  # star Form 100\n    poolKernel = 0  # set to 0 to disable pooling\n    poolStride = 0  # set to 0 to disable pooling\n    input_channels = 5\n    output_channels = 5\n##############################################\n# For Grav Intensity\n##############################################\nelif data_name == \"GravInts\":\n    S = 64\n    T = 5\n    T_in = 5\n    T_out = 5\n    DATA_PATH = \"../dataToSend/TrainingData/Intensity/\"\n    dN = 10\n    mass = \"ALL\"\n    # dt = 0.0204\n    extras = \"_intensity\"\n\n    TRAIN_PATH = f\"{DATA_PATH}Grav_M{mass}_dN{dN}{extras}.pt\"\n    TIME_PATH = f\"{DATA_PATH}Grav_M{mass}_dN{dN}{extras}.h5\"\n    N = 29\n    if mass == \"ALL\":\n        N = 902\n    data_name = f\"{data_name}{mass}_dN{dN}\"\n    input_channels = 5\n    output_channels = 5\n    modes = 8  # star Form 20\n    width = 20  # star Form 100\n    poolKernel = 0  # set to 0 to disable pooling\n    poolStride = 0  # set to 0 to disable pooling\n##############################################\n# For MHD Star Formation Data\n##############################################\nelif data_name == \"StarForm\":\n    S = 800\n    T = 1\n    T_in = 1\n    T_out = 5\n    DATA_PATH = \"../dataToSend/TrainingData/\"\n    # DATA_PATH = \"../dataToSend/FullDataTensor/\"\n    mu = \"2\"\n    dN = 10\n    sub = 1\n\n    TRAIN_PATH = f\"{DATA_PATH}density_mu{mu}_dN{dN}.pt\"\n    TIME_PATH = f\"{DATA_PATH}time_mu{mu}_dN{dN}.h5\"\n    N = 874\n    data_name = f\"{data_name}_mu{mu}\"\n    input_channels = 1\n    output_channels = 1\n    modes = 20  # star Form 20\n    width = 150  # star Form 100\n    poolKernel = 0  # set to 0 to disable pooling\n    poolStride = 0  # set to 0 to disable pooling\n\n##############################################\n# For CATS MHD Data\n##############################################\nelif data_name == \"CATS\":\n    DATA_PATH = \"../dataToSend/MHD_CATS/\"\n    TRAIN_PATH = f\"{DATA_PATH}CATS_full.pt\"\n    TIME_PATH = f\"{DATA_PATH}../CaltechData/ns_V1e-3_N5000_T50.h5\"\n\n    S = 256\n    T = 1\n    T_in = 1\n    T_out = 5\n\n    input_channels = 1\n    output_channels = 1\n    modes = 12\n    width = 48\n    N = 2970\n\nif positional_encoding:\n    input_channels += 2  # type: ignore\n\n##############################################\n# Data parameters\n##############################################\nsplit = [0.7, 0.2, 0.1]\n\n# calculate the size of the output of the pooling layer from\n# https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html\nif poolKernel &gt; 0:\n    S = math.floor((S - poolKernel) / poolStride + 1)\n\n##############################################\n# Neural network parameters\n##############################################\nNN = \"RNN\"  # \"FNO2d\", \"MNO\", \"FNO3d\" or \"CNL2d\"\nfactorization = None\nrank = 0.001\nnorm = \"group_norm\"\ng = (1, 1)\nskip_type = \"soft-gating\"  # \"identity\", \"linear\" or \"soft-gating\"\nif diss_reg:\n    radius = 156.25 * S  # radius of inner ball\n    scale_down = 0.1  # rate at which to linearly scale down inputs\n    loss_weight = 0.01 * S**2  # normalized by L2 norm in function space\n##############################################\n# Training parameters\n##############################################\nlr = 0.0001\nweight_decay = 2e-4\nepochs = 5\nbatch_size = 100\noptimizer = \"Adam\"\nscheduler_step = 10\ncosine_step = 1\nscheduler_gamma = 0.5\nif NN == \"FNO3d\" or NN == \"CNL2d\":\n    d = 3\nelse:\n    d = 2\nif loss_name == \"LpLoss\":\n    loss_fn = LpLoss(d=d, p=2, reduce_dims=(0, 1), reductions=\"mean\")\nelif loss_name == \"H1Loss\":\n    loss_fn = H1Loss(d=d, reduce_dims=(0, 1), reductions=\"mean\")\n\n\nfile = \"output.log\"\nlog_interval = 100\n</code></pre>"},{"location":"input_file/#dataset","title":"Dataset","text":"<p>The dataset is first specified by the variable <code>data_name</code> which specifies the dataset used.  This only specifies the type of data and further parameters are needed to specify the exact file to load. The current options are:</p> <ul> <li><code>'NS-Caltech'</code></li> <li><code>'StarForm'</code></li> <li><code>'GravColl'</code></li> <li><code>'CATS'</code></li> </ul>"},{"location":"input_file/#variables-for-all-datasets","title":"Variables for all datasets","text":"<p>The variables consistent for all datasets are shown in the table below. For typical values see the example input file.</p> Variable Type Description Requirements <code>DATA_PATH</code> <code>str</code> Path to the data <code>TRAIN_PATH</code> <code>str</code> Path to the training data file <code>TIME_PATH</code> <code>str</code> Path to the time data file <code>log</code> <code>bool</code> Whether to take the log10 of the data. This is only applied to the density. <code>S</code> <code>int</code> Size of the grid <code>N</code> <code>int</code> Number of samples/trajectories <code>input_channels</code> <code>int</code> Number of input channels for the neural operator <code>output_channels</code> <code>int</code> Number of output channels for the neural operator <code>modes</code> <code>int</code> Number of modes to keep in the neural operator <code>width</code> <code>int</code> Width of the neural network <code>T</code> <code>int</code> Number of time steps to predict <code>T_in</code> <code>int</code> Number of time steps to use as input <code>poolKernel</code> <code>int</code> Kernel size for average pooling. Must be a divisor of of <code>S</code> <code>poolStride</code> <code>int</code> Stride for pooling. Must be a divisor of of <code>S</code>. Best to be the same value as <code>poolKernel</code>"},{"location":"input_file/#variables-for-specific-datasets","title":"Variables for specific datasets","text":"Variable Type Description Dataset(s) <code>dN</code> <code>int</code> Number of timesteps per sample/trajectory <code>'StarForm'</code>, <code>'GravColl'</code> <code>mu</code> <code>str</code> Relative magnetic strength to the gravitational force. Used for selecting which file to read. <code>'StarForm'</code> <code>mass</code> <code>str</code> Mass of the cloud in solar masses. Used for selecting which file to read. <code>'GravColl'</code> <code>dt</code> <code>str</code> Size of the timestep in kyr. Used for selecting which file to read. <code>'GravColl'</code>"},{"location":"input_file/#neural-network","title":"Neural Network","text":""},{"location":"references/","title":"Neural Network Architectures","text":"<p>Initialize the neural Networks</p> <p>This module contains utility functions for defining and initializing neural networks used in the Neural Operator StarForm project.</p> <p>Functions:</p> Name Description <code>initializeNetwork</code> <p>dataclass) -&gt; nn.Module: Initializes the neural network model based on the input parameters.</p> <pre><code>\n</code></pre>"},{"location":"references/#src.networkUtils.initializeNetwork","title":"<code>initializeNetwork(params)</code>","text":"<p>Initialize the model Input:     params: input parameters from the input file Output:     model: torch.nn.Module</p> Source code in <code>src/networkUtils.py</code> <pre><code>def initializeNetwork(params) -&gt; nn.Module:\n    \"\"\"\n    Initialize the model\n    Input:\n        params: input parameters from the input file\n    Output:\n        model: torch.nn.Module\n    \"\"\"\n    model = nn.Module()\n    # TODO: allow MLP to be input parameter\n    if \"FNO2d\" in params.NN or params.NN == \"RNN\":\n        return FNO2d(\n            n_modes_height=params.modes,\n            n_modes_width=params.modes,\n            hidden_channels=params.width,\n            in_channels=params.input_channels,\n            out_channels=params.output_channels,\n            projection_channels=params.dim_high,\n            lifting_channels=params.dim_high,\n            use_mlp=params.use_mlp,\n            mlp_dropout=params.mlp_dropout,\n            preactivation=params.preactivation,\n            n_layers=params.n_layers,\n            skip=params.skip_type,\n            fno_block_precision=\"mixed\",\n            stablizer=\"tanh\",\n            factorization=params.factorization,\n            positional_embedding=None,\n            rank=params.rank,\n            joint_factorization=True,\n            norm=params.norm,\n        )\n    elif params.NN == \"FNO3d\" or params.NN == \"RNN3d\":\n        return FNO(\n            n_modes=(params.modes,params.modes,params.modes),\n            hidden_channels=params.width,\n            projection_channel_ratio=params.dim_high,\n            lifting_channel_ratio=params.dim_high,\n            in_channels=params.input_channels,\n            out_channels=params.output_channels,\n            use_mlp=params.use_mlp,\n            mlp_dropout=params.mlp_dropout,\n            preactivation=params.preactivation,\n            n_layers=params.n_layers,\n            skip=params.skip_type,\n            fno_block_precision=\"mixed\",\n            stablizer=\"tanh\",\n            factorization=params.factorization,\n            rank=params.rank,\n            joint_factorization=True,\n            norm=params.norm,\n            complex_data=False,\n        )\n    elif params.NN == \"MNO\":\n        return FNO2d(\n            n_modes_height=params.modes,\n            n_modes_width=params.modes,\n            hidden_channels=params.width,\n            in_channels=params.input_channels,\n            out_channels=params.output_channels,\n            use_mlp=params.use_mlp,\n            mlp_dropout=params.mlp_dropout,\n            preactivation=params.preactivation,\n            n_layers=params.n_layers,\n            skip=params.skip_type,\n        )\n    elif params.NN == \"UNet\":\n        return UNet2d(params.input_channels, params.output_channels, params.width)\n    elif params.NN == \"UNO\":\n        powers = [i for i in range(params.n_layers // 2)]\n        powers = powers + powers[::-1]\n        if params.n_layers % 2 == 1:\n            powers.insert(params.n_layers // 2, params.n_layers // 2)\n        scalings = [[1, 1, 1]] * params.n_layers\n        scalings = [[1, 1, 1]] * params.n_layers\n        if params.n_layers &gt; 2:\n            scalings[1] = [0.5, 0.5, 0.5]\n            scalings[-1] = [2, 2, 2]\n            print([params.width * 2**p for p in powers])\n            return UNO(\n                in_channels=params.input_channels,\n                out_channels=params.output_channels,\n                projection_channels=params.width*params.dim_high,\n                lifting_channels=params.width*params.dim_high,\n                uno_n_modes=[[params.modes] * params.d] * params.n_layers,\n                uno_out_channels=[params.width * 2**p for p in powers],\n                uno_scalings=scalings,\n                hidden_channels=params.width,\n                n_layers=params.n_layers,\n                channel_mlp_skip=\"linear\",\n                mlp_dropout=params.mlp_dropout,\n                preactivation=params.preactivation,\n                skip=params.skip_type,\n                complex_data=False,\n                fno_block_precision=\"mixed\", \n            )\n        elif params.n_layers == 3:\n            m = params.modes\n            w = params.width\n            return UNO(\n                in_channels=params.input_channels,\n                out_channels=params.output_channels,\n                projection_channels=params.width*params.dim_high,\n                lifting_channels=params.width*params.dim_high,\n                uno_n_modes=[[m, m, m], [m // 2, m // 2, m // 2], [m, m, m]],\n                uno_out_channels=[w, w * 3 // 2, w],\n                uno_scalings=scalings,\n                hidden_channels=params.width,\n                n_layers=params.n_layers,\n                channel_mlp_skip=\"linear\",\n                mlp_dropout=params.mlp_dropout,\n                preactivation=params.preactivation,\n                skip=params.skip_type,\n                fno_block_precision=\"mixed\",\n                complex_data=False,\n            )\n\n    return model\n</code></pre>"}]}